{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6753d463-a8c8-44e5-abbd-8339885ec91b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Connect to ADLS Gen2 (Storage Account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81db27e6-f96b-4e1e-8f2e-7d1d725ce6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to ADLS Gen2 (Storage Account)\n",
    "spark.conf.set(\n",
    "\"fs.azure.account.key.goodreadsreviews60300294.dfs.core.windows.net\",\n",
    "\"SdrUSgCnzVYmEhQn9mzu3HtSdzHfZLLnQ+2ofOm7fq4GktiUUs3bZw7qJoD8BXFqtyfzCkDbfKZI+ASt5tp6qQ==\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa9920f-9aa4-4f8c-9010-d0cb1a0ba7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the curated Gold dataset from previous Lab (Lab3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c424624-6c5b-4e94-af39-8b5bc0f0492a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_curated_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/curated_reviews/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(gold_curated_path)\n",
    "\n",
    "print(\"Total rows in curated_reviews:\", df.count())\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42df6e62-2736-48ff-b81b-ed2a01f9e986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from pyspark.sql.functions import col, length, udf, split, size\n",
    "from pyspark.sql.types import StringType\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceb6565-1211-4279-ad31-8c7c7be5443d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitting the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d977e41e-0fa4-4f81-8e8d-0e8b65188b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split curated dataset into (train / val / test)\n",
    "\n",
    "# Create 3 splits\n",
    "train_df, val_df, test_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "print(\"Train rows:\", train_df.count())\n",
    "print(\"Val rows:  \", val_df.count())\n",
    "print(\"Test rows: \", test_df.count())\n",
    "\n",
    "# Base path for features_v2\n",
    "base_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2/\"\n",
    "\n",
    "train_path = base_path + \"train\"\n",
    "val_path   = base_path + \"val\"\n",
    "test_path  = base_path + \"test\"\n",
    "\n",
    "# Save each split as Delta\n",
    "(train_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(train_path))\n",
    "\n",
    "(val_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(val_path))\n",
    "\n",
    "(test_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(test_path))\n",
    "\n",
    "print(\"✅ Saved train/val/test splits under features_v2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d8a7d4-1c30-4166-9b6b-22559e46b049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## fast check on the files are there \n",
    "#### not mentioned in the lab, but i have do it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a07291-4ecf-4709-b903-82422cd1bb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(base_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a50c808-87c2-440e-8163-8e37628b7d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the TRAIN Split from the Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdc813a-53cc-405e-8b58-b3a2258e7fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Load TRAIN split from features_v2\n",
    "base_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2/\"\n",
    "train_path = base_path + \"train\"\n",
    "\n",
    "train_df = spark.read.format(\"delta\").load(train_path)\n",
    "\n",
    "print(\"Original TRAIN rows:\", train_df.count())\n",
    "train_df.select(\"review_id\", \"review_text\").show(5, truncate=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb8df00-6064-4f0a-9074-baa82c54fd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Function for clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6ce05b-b1a2-47f5-904d-e2d3c11669c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Define a Python function to clean text\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    \n",
    "    # to lowercase\n",
    "    t = text.lower()\n",
    "    \n",
    "    # replace URLs with placeholder\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" url \", t)\n",
    "    \n",
    "    # replace numbers with placeholder\n",
    "    t = re.sub(r\"\\d+\", \" number \", t)\n",
    "    \n",
    "    # remove emojis / non-ASCII characters\n",
    "    t = re.sub(r\"[^\\x00-\\x7F]+\", \" \", t)\n",
    "    \n",
    "    # remove punctuation (keep only letters and spaces)\n",
    "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
    "    \n",
    "    # collapse multiple spaces -> single space\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    \n",
    "    # trim spaces\n",
    "    t = t.strip()\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ffa5616-e0b2-43e3-a56f-4eb54e8dce16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply Text Cleaning and Normalize Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2423ff-83b1-4578-b8d4-b4bc6ac2c356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Register as UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# 4) Apply cleaning to review_text\n",
    "train_clean = (\n",
    "    train_df\n",
    "    .withColumn(\"review_text_clean\", clean_text_udf(col(\"review_text\")))\n",
    ")\n",
    "\n",
    "# 5) Drop very short or empty reviews (< 10 characters)\n",
    "train_clean = train_clean.filter(\n",
    "    (col(\"review_text_clean\").isNotNull()) &\n",
    "    (length(col(\"review_text_clean\")) >= 10)\n",
    ")\n",
    "\n",
    "print(\"Cleaned TRAIN rows:\", train_clean.count())\n",
    "\n",
    "# 6) Quick preview of cleaned text\n",
    "train_clean.select(\"review_id\", \"review_text\", \"review_text_clean\") \\\n",
    "           .show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d4af6c-a878-4cff-94e7-d6347d179d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Extract Basic Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ec7c14-ae17-4579-bc05-e6e1347f4522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5A: Add basic text features to train_clean\n",
    "\n",
    "# 1) Number of characters\n",
    "train_basic = train_clean.withColumn(\n",
    "    \"review_length_chars\", length(col(\"review_text_clean\"))\n",
    ")\n",
    "\n",
    "# 2) Number of words\n",
    "train_basic = train_basic.withColumn(\n",
    "    \"review_length_words\", size(split(col(\"review_text_clean\"), \" \"))\n",
    ")\n",
    "\n",
    "print(\"Rows after adding basic features:\", train_basic.count())\n",
    "\n",
    "train_basic.select(\n",
    "    \"review_id\",\n",
    "    \"review_text_clean\",\n",
    "    \"review_length_chars\",\n",
    "    \"review_length_words\"\n",
    ").show(10, truncate=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda4ba95-cc1b-4512-82b6-542a3f44638d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Installing NLTK + VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cac304f-5463-4c23-90c1-503a7a06c55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bb85527-bce9-4bdd-92d0-71a33e893167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sentiment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d60798-24fe-43a2-82b0-d815ec237767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5B: Sentiment features using NLTK VADER\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# 1) Create VADER analyzer (on driver)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 2) Define a function that returns a struct with 4 scores\n",
    "def get_sentiment_scores(text):\n",
    "    if text is None:\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return (\n",
    "        float(scores[\"pos\"]),\n",
    "        float(scores[\"neg\"]),\n",
    "        float(scores[\"neu\"]),\n",
    "        float(scores[\"compound\"])\n",
    "    )\n",
    "\n",
    "# 3) Define schema for the struct\n",
    "sentiment_schema = StructType([\n",
    "    StructField(\"pos\", DoubleType(), nullable=False),\n",
    "    StructField(\"neg\", DoubleType(), nullable=False),\n",
    "    StructField(\"neu\", DoubleType(), nullable=False),\n",
    "    StructField(\"compound\", DoubleType(), nullable=False),\n",
    "])\n",
    "\n",
    "# 4) Register UDF\n",
    "sentiment_udf = udf(get_sentiment_scores, sentiment_schema)\n",
    "\n",
    "# 5) Apply to the cleaned text column\n",
    "train_sent = (\n",
    "    train_basic\n",
    "    .withColumn(\"sentiment\", sentiment_udf(col(\"review_text_clean\")))\n",
    "    .withColumn(\"sentiment_pos\", col(\"sentiment.pos\"))\n",
    "    .withColumn(\"sentiment_neg\", col(\"sentiment.neg\"))\n",
    "    .withColumn(\"sentiment_neu\", col(\"sentiment.neu\"))\n",
    "    .withColumn(\"sentiment_compound\", col(\"sentiment.compound\"))\n",
    "    .drop(\"sentiment\")  # drop struct after expanding\n",
    ")\n",
    "\n",
    "print(\"Rows after adding sentiment features:\", train_sent.count())\n",
    "\n",
    "train_sent.select(\n",
    "    \"review_id\",\n",
    "    \"review_text_clean\",\n",
    "    \"sentiment_pos\",\n",
    "    \"sentiment_neg\",\n",
    "    \"sentiment_neu\",\n",
    "    \"sentiment_compound\"\n",
    ").show(10, truncate=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b982bd5e-9d9b-412a-a388-cd213cffae10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b591b11e-45b4-48ba-a510-6ae676d5e4e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install libraries\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14550829-7fd3-4dca-811a-a3d9d3def4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6cce74-26ff-4233-9d92-c8ce9dceadcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5C: TF-IDF features on review_text_clean (TRAIN)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1) Start from train_sent (has cleaned text + basic + sentiment features)\n",
    "#    If your last variable name is different, adjust here.\n",
    "train_for_tfidf = train_sent.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# OPTIONAL: if dataset is too large, uncomment this line to limit rows for TF-IDF\n",
    "# train_for_tfidf = train_for_tfidf.limit(50000)\n",
    "\n",
    "# 2) Move the needed columns to Pandas\n",
    "pdf = train_for_tfidf.select(\"row_id\", \"review_text_clean\").limit(100).toPandas()\n",
    "\n",
    "# 3) Prepare TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100,       # top 5000 terms\n",
    "    ngram_range=(1, 2),      # unigrams + bigrams\n",
    "    stop_words=\"english\"     # remove common English stopwords\n",
    ")\n",
    "\n",
    "# Replace NaN with empty string just in case\n",
    "texts = pdf[\"review_text_clean\"].fillna(\"\")\n",
    "\n",
    "# 4) Fit TF-IDF on TRAIN text\n",
    "X = tfidf.fit_transform(texts)\n",
    "\n",
    "# 5) Build a Pandas DataFrame with TF-IDF features\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_cols = [f\"tfidf_{w}\" for w in feature_names]\n",
    "\n",
    "tfidf_pdf = pd.DataFrame(X.toarray(), columns=tfidf_cols)\n",
    "tfidf_pdf[\"row_id\"] = pdf[\"row_id\"].values\n",
    "\n",
    "# 6) Convert TF-IDF features back to Spark\n",
    "tfidf_spark = spark.createDataFrame(tfidf_pdf)\n",
    "\n",
    "# 7) Join TF-IDF features back to the original TRAIN dataframe\n",
    "train_tfidf = train_for_tfidf.join(tfidf_spark, on=\"row_id\", how=\"inner\")\n",
    "\n",
    "print(\"Rows in train_tfidf:\", train_tfidf.count())\n",
    "print(\"Number of TF-IDF feature columns:\", len(tfidf_cols))\n",
    "\n",
    "# Show a sample with a few TF-IDF columns\n",
    "sample_cols = [\"review_id\", \"review_text_clean\"] + tfidf_cols[:10]\n",
    "\n",
    "train_tfidf.select(sample_cols).show(5, truncate=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5362047-6a87-4212-9881-e8226174f9ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a5452f-2082-488e-bdec-fe26e64e5310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d9e939-8ac4-416a-8a53-7509d09e8226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sentence-BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4ad024-a3bb-4482-834d-4a983acf72cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5D: Sentence-BERT Embeddings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load the model once (on driver)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')   # 384-dimensional embeddings\n",
    "\n",
    "# 2) Add row_id (if not present) for joining later\n",
    "train_embed = train_tfidf.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# 3) Select only what we need\n",
    "pdf = train_embed.select(\"row_id\", \"review_text_clean\").limit(100).toPandas()\n",
    "\n",
    "# Replace any None with \" \" to avoid errors\n",
    "pdf[\"review_text_clean\"] = pdf[\"review_text_clean\"].fillna(\" \")\n",
    "\n",
    "# 4) Encode in batches to avoid GPU/CPU memory issues\n",
    "batch_size = 256\n",
    "embeddings_list = []\n",
    "\n",
    "for i in range(0, len(pdf), batch_size):\n",
    "    batch_texts = pdf[\"review_text_clean\"].iloc[i:i+batch_size].tolist()\n",
    "    batch_embeddings = model.encode(batch_texts)\n",
    "    embeddings_list.append(batch_embeddings)\n",
    "\n",
    "# Stack all batches together\n",
    "embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "# 5) Build Pandas DataFrame\n",
    "embed_cols = [f\"embed_{i}\" for i in range(embeddings.shape[1])]\n",
    "embed_pdf = pd.DataFrame(embeddings, columns=embed_cols)\n",
    "\n",
    "# Add row_id back\n",
    "embed_pdf[\"row_id\"] = pdf[\"row_id\"].values\n",
    "\n",
    "# 6) Convert back to Spark\n",
    "embed_spark = spark.createDataFrame(embed_pdf)\n",
    "\n",
    "# 7) Join embeddings with full training dataset\n",
    "train_final = train_embed.join(embed_spark, on=\"row_id\", how=\"inner\")\n",
    "\n",
    "print(\"Rows in final train dataset:\", train_final.count())\n",
    "print(\"Embedding columns:\", len(embed_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b05a72-1351-4d56-9e72-1240cc318693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sanitize Column Names for Delta Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac72115e-fb69-47ae-8f95-7b630282f602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start from your final training dataframe\n",
    "clean_train_final = train_final\n",
    "\n",
    "# Function to make column names Delta-safe\n",
    "def sanitize_col(name):\n",
    "    # remove leading/trailing spaces\n",
    "    name = name.strip()\n",
    "    # replace invalid characters with underscore\n",
    "    name = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", name)\n",
    "    # collapse multiple underscores\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    return name\n",
    "\n",
    "# Apply to all columns\n",
    "for old_name in clean_train_final.columns:\n",
    "    new_name = sanitize_col(old_name)\n",
    "    if new_name != old_name:\n",
    "        clean_train_final = clean_train_final.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "# Optional: quickly inspect some of the TF-IDF columns\n",
    "print(\"Sample columns after sanitizing:\")\n",
    "print([c for c in clean_train_final.columns if c.startswith(\"tfidf_\")][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bae5de76-b2a5-431f-89d9-d40e6bb5a070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save final TRAIN features to Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17fa71a5-cc07-4617-8755-4700f1dbae8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 6 & 7: Save final TRAIN features to Gold layer\n",
    "\n",
    "features_train_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2/train_features/\"\n",
    "\n",
    "(clean_train_final\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .save(features_train_path))\n",
    "\n",
    "print(\"✅ Saved final training features to:\")\n",
    "print(features_train_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41285d11-26be-407c-a7f2-9158ddc3f12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Register final TRAIN features as a managed Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae61ee0-92ca-423f-ac62-affeb10bccc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_train_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2/train_features/\"\n",
    "\n",
    "# 1) Read the Delta files from ADLS\n",
    "df_features = spark.read.format(\"delta\").load(features_train_path)\n",
    "\n",
    "# 2) Save as a managed table in the 'default' schema\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS goodreads_train_features\")\n",
    "\n",
    "df_features.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.goodreads_train_features\")\n",
    "\n",
    "# 3) Quick checks\n",
    "spark.sql(\"SELECT COUNT(*) AS rows FROM default.goodreads_train_features\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  SELECT \n",
    "    review_id,\n",
    "    rating,\n",
    "    review_length_words,\n",
    "    sentiment_compound\n",
    "  FROM default.goodreads_train_features\n",
    "  ORDER BY review_length_words DESC\n",
    "  LIMIT 10\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d07b0be0-30c8-4db9-b748-31de73d34ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## save it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02c59ba-4e48-4660-b2b1-bdd86b6ec779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2_sample/\"\n",
    "\n",
    "df_features.coalesce(1).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "print(\"Saved CSV to:\", csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95996d56-910a-4634-8209-83e5f7f98298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## rename the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce5ef19-37e3-4900-a574-fe2c340d18f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_file = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2_sample/part-00000-tid-8958894957276436688-1f9f5bc5-e9cb-4b94-a958-42770b6faf3c-3304-1-c000.csv\"\n",
    "new_file = \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/gold/features_v2_sample/data_lab4.csv\"\n",
    "\n",
    "dbutils.fs.mv(old_file, new_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0eb5d9-fcd1-4446-b8aa-02b177529d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## check the folder what it have inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc52236-7c46-43cf-a9b6-1e66a5b83bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(csv_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "goodreads_text_features",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
