{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871fdeab-157c-4052-8171-fa4fca05b5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Connect to ADLS Gen2 (Storage Account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a53ed2-defd-469a-9cd7-7a5d5cf600ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your actual storage account name and key\n",
    "spark.conf.set(\n",
    "\"fs.azure.account.key.goodreadsreviews60300294.dfs.core.windows.net\",\n",
    "\"SdrUSgCnzVYmEhQn9mzu3HtSdzHfZLLnQ+2ofOm7fq4GktiUUs3bZw7qJoD8BXFqtyfzCkDbfKZI+ASt5tp6qQ==\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e31fd9a-efbb-4e2d-9c6b-499bec04a478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Silver Parquet (Books & Authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b2d1b3a-27c8-447f-80e2-cef4108abf92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the books dataset from the silver layer\n",
    "books = spark.read.parquet(\n",
    "\"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/processed/books/\"\n",
    ")\n",
    "# Load the authors dataset from the silver layer\n",
    "authors = spark.read.parquet(\n",
    "\"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/processed/authors/\"\n",
    ")\n",
    "# Display the first few records to confirm the data was loaded correctly\n",
    "books.show(5)\n",
    "authors.show(5)\n",
    "# Display the columns and their data types to verify the schema\n",
    "books.printSchema()\n",
    "authors.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ade7aa-5816-4536-b9b4-27ad64e5a4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read reviews (Parquet) and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36b5f5f1-64ef-430b-9a0e-9bc7193830b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, length, trim, count, when\n",
    "# Read raw (uncleaned) reviews from the silver layer\n",
    "reviews = spark.read.parquet(\n",
    "\"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/processed/reviews/\"\n",
    ")\n",
    "# Peek at rows and schema\n",
    "reviews.show(5, truncate=False)\n",
    "reviews.printSchema()\n",
    "# Basic profiling: counts and potential issues\n",
    "total_rows = reviews.count()\n",
    "null_review_id = reviews.filter(col(\"review_id\").isNull()).count()\n",
    "null_book_id = reviews.filter(col(\"book_id\").isNull()).count()\n",
    "null_user_id = reviews.filter(col(\"user_id\").isNull()).count()\n",
    "null_rating = reviews.filter(col(\"rating\").isNull()).count()\n",
    "empty_text = reviews.filter( (col(\"review_text\").isNull()) | (trim(col(\"review_text\")) ==\n",
    "\"\") ).count()\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"NULL review_id: {null_review_id}, NULL book_id: {null_book_id}, NULL user_id:{null_user_id}, NULL rating: {null_rating}\")\n",
    "print(f\"Empty/NULL review_text: {empty_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1cd529-8888-4ec4-a7f1-d713912708d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean reviews by removing problematic rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e62d62-14c8-4808-83b6-8b2e4e7c4ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, length\n",
    "# Start from the existing Parquet-loaded DataFrame\n",
    "# (Assumes you already did: reviews = spark.read.parquet(\".../processed/reviews/\"))\n",
    "df = reviews\n",
    "# 1) Drop rows missing critical keys\n",
    "df = df.filter(\n",
    "col(\"review_id\").isNotNull() &\n",
    "col(\"book_id\").isNotNull() &\n",
    "col(\"user_id\").isNotNull()\n",
    ")\n",
    "# 2) Enforce rating to be integer in [1..5]\n",
    "df = df.withColumn(\"rating_int\", col(\"rating\").cast(\"int\"))\n",
    "df = df.filter(\n",
    "col(\"rating_int\").isNotNull() &\n",
    "(col(\"rating_int\") >= 1) &\n",
    "(col(\"rating_int\") <= 5)\n",
    ")\n",
    "# 3) Normalize text; drop empty or ultra-short reviews (<10 chars after trim)\n",
    "df = df.withColumn(\"review_text\", trim(col(\"review_text\")))\n",
    "df = df.filter(\n",
    "col(\"review_text\").isNotNull() &\n",
    "(length(col(\"review_text\")) >= 10)\n",
    ")\n",
    "# 4) De-duplicate by review_id (keep arbitrary first; refine if you have timestamps)\n",
    "df = df.dropDuplicates([\"review_id\"])\n",
    "# 5) Select final shape\n",
    "reviews_clean = df.select(\n",
    "\"review_id\",\n",
    "\"book_id\",\n",
    "\"user_id\",\n",
    "col(\"rating_int\").alias(\"rating\"),\n",
    "\"review_text\",\n",
    "\"n_votes\",\n",
    "\"date_added\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda3a24f-fc72-4804-84dd-4de6000f71a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Persist Cleaned Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7c6a1c-063d-419e-85ab-864f54d53163",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the cleaned reviews back to the silver layer (overwrite)\n",
    "reviews_clean.write.mode(\"overwrite\").parquet(\n",
    "\"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/processed/reviews/\"\n",
    ")\n",
    "# Sanity check: re-read from disk and inspect schema and a few rows\n",
    "reviews_verified = spark.read.parquet(\n",
    "\"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/processed/reviews/\"\n",
    ")\n",
    "reviews_verified.printSchema()\n",
    "reviews_verified.show(5, truncate=False)\n",
    "print(f\"Written cleaned rows: {reviews_verified.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1015f62-3dda-4f29-aae1-f1f8cd836c68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## HomeWork Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3f4979-2d9f-4286-b37e-be34ef62f072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# HomeWork Part 1\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assign aliases to DataFrames for easier column reference\n",
    "b = books.alias(\"b\")\n",
    "a = authors.alias(\"a\")\n",
    "r = reviews_verified.alias(\"r\")\n",
    "\n",
    "# Join the dataframes\n",
    "ba = b.join(a, on='author_id', how='inner')\n",
    "joined = r.join(ba, on='book_id', how='inner')\n",
    "joined = r.join(ba, on='book_id', how='inner')\n",
    "\n",
    "# curated reviews gold \n",
    "curated_reviews_gold = joined.select(\n",
    "    col(\"r.review_id\"),\n",
    "    col(\"r.book_id\"),\n",
    "    col(\"b.title\"),\n",
    "    col(\"a.author_id\"),\n",
    "    col(\"a.name\"),\n",
    "    col(\"r.user_id\"),\n",
    "    col(\"r.rating\"),\n",
    "    col(\"r.review_text\"),\n",
    "    col(\"b.language_code\").alias(\"language\"),\n",
    "    col(\"r.n_votes\"),\n",
    "    col(\"r.date_added\")\n",
    ")\n",
    "\n",
    "# Print the schema and display a few rows to verify the result\n",
    "print(\"Curated Reviews Gold DataFrame Schema:\")\n",
    "curated_reviews_gold.printSchema()\n",
    "print(\"Curated Reviews Gold DataFrame Sample Rows:\")\n",
    "curated_reviews_gold.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83bf14a8-8e12-42ef-b130-512f7e346ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(curated_reviews_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5151995-77d5-4f83-8854-7c7c2df0b5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use a catalog/schema you can write to\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "\n",
    "# Persist as a managed Delta table\n",
    "curated_reviews_gold.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.curated_reviews\")\n",
    "\n",
    "# Verify\n",
    "spark.sql(\"SELECT COUNT(*) AS rows FROM default.curated_reviews\").show()\n",
    "spark.sql(\"\"\"\n",
    "  SELECT title, name, rating, language\n",
    "  FROM default.curated_reviews\n",
    "  ORDER BY date_added DESC\n",
    "  LIMIT 10\n",
    "\"\"\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "359c784b-9779-4e50-a7cd-a459274c4c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1️⃣ Load the curated_reviews table from the default schema\n",
    "curated_reviews = spark.table(\"default.curated_reviews\")\n",
    "\n",
    "# 2️⃣ Select the first 100 rows\n",
    "sample_df = curated_reviews.limit(100)\n",
    "\n",
    "# 3️⃣ Define the output path (organized under 'processed/reviews')\n",
    "sample_output_path = (\n",
    "    \"abfss://lakehouse@goodreadsreviews60300294.dfs.core.windows.net/\"\n",
    "    \"processed/reviews/sample_curated_data/\"\n",
    ")\n",
    "\n",
    "# 4️⃣ Write the small dataset as a single CSV file (overwrite if exists)\n",
    "sample_df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(sample_output_path)\n",
    "\n",
    "print(f\"✅ Sample CSV saved to: {sample_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5827922216031332,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Preprocessing_Part_1.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
